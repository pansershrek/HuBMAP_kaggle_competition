{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from medpy.io import load\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "import torchgeometry as tgm\n",
    "import torch.nn as nn\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "from skimage import transform\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af5e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 1717\n",
    "    base_path = \"./hubmap-organ-segmentation\"\n",
    "    batch_size = 4\n",
    "    \n",
    "    val_size = 0.25\n",
    "    \n",
    "    epoch = 30\n",
    "    lr = 3e-3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    rescaling_factor = 3\n",
    "    \n",
    "    crop_size = 256\n",
    "    crop_amount = 8\n",
    "    \n",
    "    #base_path = \"../input/hubmap-organ-segmentation\" #UNCOMMIT FOR KAGGLE\n",
    "    train_metadata = os.path.join(base_path, \"train.csv\")\n",
    "    test_metadata = os.path.join(base_path, \"test.csv\")\n",
    "    train_images = os.path.join(base_path, \"train_images\")\n",
    "    test_images = os.path.join(base_path, \"test_images\")\n",
    "    \n",
    "    model_path = \"model\"\n",
    "    \n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8b4f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    " \n",
    "def rle2mask(mask_rle, shape=(1600,256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (width,height) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8afa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image, mask = data[\"image\"], data[\"mask\"]\n",
    "        \n",
    "        h, w = image.shape[1:]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[:, top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        mask = mask[:, top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return {\"image\": image, \"mask\": mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, root_dir, metadata,\n",
    "        image_transforms=None, mask_transforms=None,\n",
    "        rescaling_factor = config.rescaling_factor,\n",
    "        crop_size = 1, crop_amount = 1, val = False\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.crop = RandomCrop(crop_size)\n",
    "        self.crop_amount = crop_amount\n",
    "        self.val = val\n",
    "        self.new_voxel_spacing = np.array(\n",
    "            [\n",
    "                3,\n",
    "                0.171 * rescaling_factor,\n",
    "                0.171 * rescaling_factor\n",
    "            ]\n",
    "        )\n",
    "        cur_ids = set(metadata[\"id\"])\n",
    "        files = [\n",
    "            x for x in os.listdir(self.root_dir) if int(\n",
    "                x.split(\".\")[0]\n",
    "            ) in cur_ids\n",
    "        ]\n",
    "        self.idx2name = {\n",
    "            x: os.path.join(self.root_dir, y)\n",
    "            for x, y in enumerate(files)\n",
    "        }\n",
    "        self.name2idx = {\n",
    "            int(y.split(\".\")[1].split(\"/\")[-1]): x\n",
    "            for x, y in self.idx2name.items()\n",
    "        }\n",
    "        self.idx2meta = {\n",
    "            self.name2idx[x[1][\"id\"]]: {\n",
    "                \"mask\": None,\n",
    "                \"pixel_size\": x[1][\"pixel_size\"],\n",
    "                \"tissue_thickness\": x[1][\"tissue_thickness\"],\n",
    "            }\n",
    "            for x in metadata.iterrows()\n",
    "        }\n",
    "        for x in metadata.iterrows():\n",
    "            if x[1].get(\"rle\") is not None:\n",
    "                self.idx2meta[self.name2idx[x[1][\"id\"]]][\"mask\"] = rle2mask(\n",
    "                    x[1][\"rle\"],\n",
    "                    (x[1][\"img_width\"], x[1][\"img_height\"])\n",
    "                )\n",
    "    \n",
    "    def get_voxel_size(self, idx):\n",
    "        image = load(self.idx2name[idx])\n",
    "        return np.array(image[1].get_voxel_spacing())\n",
    "    \n",
    "    def zoom(self, data, zoom_scale):\n",
    "        image, mask = data['image'], data['mask']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        h *= zoom_scale\n",
    "        w *= zoom_scale\n",
    "        img = transform.resize(image, (h, w))\n",
    "        mask = transform.resize(mask, (h, w))\n",
    "        return {'image': img, 'mask': mask}\n",
    "    \n",
    "    def to_image(self, data):\n",
    "        return {\n",
    "            'image': Image.fromarray((data['image'] * 255).astype(np.uint8)),\n",
    "            'mask': Image.fromarray((data['mask'] * 255).astype(np.uint8))\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2name) * self.crop_amount if not self.val else len(self.idx2name)\n",
    "    \n",
    "    def __getitem__(self, idx_):\n",
    "        idx = idx_ // self.crop_amount\n",
    "        image = tiff.imread(self.idx2name[idx])\n",
    "        mask = self.idx2meta[idx][\"mask\"]\n",
    "        #TODO Use pixel_size and tissue_thickness here\n",
    "        ans = {\"image\": image, \"mask\": mask}\n",
    "        ans = self.zoom(ans, self.idx2meta[idx][\"pixel_size\"])\n",
    "        ans = self.to_image(ans)\n",
    "        \n",
    "        if self.image_transforms:\n",
    "            ans[\"image\"] = self.image_transforms(ans[\"image\"])\n",
    "        if self.mask_transforms:\n",
    "            ans[\"mask\"] = self.mask_transforms(ans[\"mask\"])\n",
    "        \n",
    "        return self.crop(ans) if not self.val else ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feeef02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.backbone = torch.hub.load(\n",
    "        #    'mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "        #    in_channels=3, out_channels=1, init_features=32, pretrained=False\n",
    "        #)\n",
    "        self.backbone = torch.hub.load(\n",
    "            'milesial/Pytorch-UNet', 'unet_carvana', pretrained=True, scale=0.5\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ans = self.backbone(x)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5f9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(logit, target):\n",
    "    pred = torch.sigmoid(logit)\n",
    "    \n",
    "    sum_dims = list(range(1, target.dim()))\n",
    "    dice = 2 * torch.sum(pred * target, dim = sum_dims) / torch.sum(pred ** 2 + target ** 2, dim = sum_dims)\n",
    "    loss = 1 - dice\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2018f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234870b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_source = pd.read_csv(config.train_metadata)\n",
    "df_train = df_train_source.sample(frac=1, random_state=config.seed).reset_index(drop=True)[:300]\n",
    "df_val = df_train_source.sample(frac=1, random_state=config.seed).reset_index(drop=True)[300:]\n",
    "\n",
    "df_test = pd.read_csv(config.test_metadata)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_transforms = [\n",
    "    transforms.ToTensor(),\n",
    "]\n",
    "image_transforms = transforms.Compose(\n",
    "    general_transforms + [\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "mask_transforms = transforms.Compose(general_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaselineDataset(\n",
    "    config.train_images, df_train,\n",
    "    image_transforms = image_transforms,\n",
    "    mask_transforms = mask_transforms,\n",
    "    crop_size = config.crop_size,\n",
    "    crop_amount = config.crop_amount,\n",
    "    val = False\n",
    ")\n",
    "val_dataset = BaselineDataset(\n",
    "    config.train_images, df_val,\n",
    "    image_transforms = image_transforms,\n",
    "    mask_transforms = mask_transforms,\n",
    "    val = True\n",
    ")\n",
    "test_dataset = BaselineDataset(\n",
    "    config.test_images, df_test,\n",
    "    image_transforms = image_transforms,\n",
    "    mask_transforms = mask_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=config.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=config.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2831fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c25b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel()\n",
    "model = model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    config.lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17dcbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, loss_fn, val_dataloader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for x in val_dataloader:\n",
    "            mask_pred = model(x[\"image\"].to(device))\n",
    "            loss = loss_fn(\n",
    "                mask_pred, \n",
    "                x[\"mask\"].to(device)\n",
    "            )\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    return sum(val_loss) / len(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17e224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9eaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_dataloader, val_dataloader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for step, x in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            mask_pred = model(x[\"image\"].to(device))\n",
    "            loss = loss_fn(\n",
    "                mask_pred,\n",
    "                x[\"mask\"].to(device)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            #print(f\"Step {step} Train loss: {sum(train_loss) / len(train_loss)}\")\n",
    "        val_loss = val(model, loss_fn, val_dataloader, device)\n",
    "        \n",
    "        print(f\"Train loss: {sum(train_loss) / len(train_loss)}\")\n",
    "        print(f\"Val loss: {val_loss}\")\n",
    "        torch.save(model.state_dict(), os.path.join(config.model_path, f\"model_{epoch}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, dice, train_dataloader, val_dataloader, config.epoch, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705440a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.idx2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9028a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
